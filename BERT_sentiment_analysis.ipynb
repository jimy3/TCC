{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259422ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from textblob import TextBlob\n",
    "import datashader as ds\n",
    "import colorcet as cc\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import re\n",
    "plt.style.use('fivethirtyeight')\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import emoji\n",
    "import os\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'S&P500 lang:en until:2022-12-31 since:2022-07-01'\n",
    "tweets=[]\n",
    "limit=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        tweets.append([tweet.date, tweet.id, tweet.rawContent, tweet.likeCount, tweet.retweetCount, tweet.user.followersCount])\n",
    "    \n",
    "df = pd.DataFrame(tweets, columns=['Date', 'Id', 'Tweet', 'Like', 'Retweet', 'Followers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20033e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('tweets_07_01_12_31_newall.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52e9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess_Tweets(data):\n",
    "\t\t\n",
    "\tdata['Text_Cleaned'] = data['Tweet'].str.lower()\n",
    "\n",
    "\t## FIX HYPERLINKS\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'https?:\\/\\/.*[\\r\\n]*', ' ',regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'www.*[\\r\\n]*', ' ',regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('https', '', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX INDIVIDUAL SYMBOLS \n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(': ', ' ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(', ', ' ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('. ', ' ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[;\\n~]', ' ', regex=True)\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(\"[]'â€¦*™|]\", '', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[[()!?\"]', '', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('_', '', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('w/', ' with ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('f/', ' for ', regex=False)\n",
    "    \n",
    "\t## FIX EMOJIS\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(':)', '', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(':-)', '', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(':(', '', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(':-(', '', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('0_o', '', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(';)', '', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('=^.^=', '', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX % SYMBOL\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('%', ' percent ', regex=False)\n",
    "    \n",
    "\t## FIX 【】 SYMBOL\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('】', '', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('【', '', regex=False)    \n",
    "\n",
    "\n",
    "\t## FIX & SYMBOL\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' & ', ' and ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('&amp', ' and ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('&gt', ' greater than ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('cup&handle', 'cup and handle', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('c&h', 'cup and handle', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('head&shoulders', 'head and shoulders', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('h&s', 'head and shoulders', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('point&figure', 'point and figure', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('p&f', 'point and figure', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('s&p', 'SP500', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('q&a', 'question and answer', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('&', ' and ', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX USER TAGS AND HASTAGS\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('@[a-z0-9]+', '', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('#[a-z0-9]+', '', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('@', '', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('#', '', regex=False)\n",
    "\t   \n",
    "\t\t\n",
    "\t## FIX EMBEDDED COMMAS AND PERIODS    \n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z]),([a-z])', r'\\1 \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]),([0-9])', r'\\1\\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])[+]+', r'\\1 ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(',', '', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('u.s.', ' us ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('\\.{2,}', ' ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z])\\.([a-z])', r'\\1 \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('pdating', 'updating', regex=False) \n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z])\\.', r'\\1 ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'\\.([a-z])', r' \\1', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' . ', ' ', regex=False)\n",
    "\t\t\n",
    "\n",
    "\t## FIX + SYMBOL\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'[+]([0-9])', r'positive \\1', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('c+h', 'cup and handle', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('h+s', 'head and shoulders', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('cup+handle', 'cup and handle', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' + ', ' and ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('+ ', ' ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z])[+]([a-z])', r'\\1 and \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('+', '', regex=False)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\n",
    "\t## FIX - SYMBOL\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z])[-]+([a-z])', r'\\1 \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z]) - ([a-z])', r'\\1 to \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]) -([0-9\\.])', r'\\1 to \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r' [-]([0-9])', r' negative \\1', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])-([0-9\\.])', r'\\1 to \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]) - ([0-9\\.])', r'\\1 to \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9a-z])-([0-9a-z])', r'\\1 \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[-]+[>]', ' ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' [-]+ ', ' ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('-', ' ', regex=False)\n",
    "\n",
    "\n",
    "\n",
    "\t## FIX $ SYMBOL\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[$][0-9\\.]', ' dollars ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('$', '', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX = SYMBOL\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('=', ' equals ', regex=False)\n",
    "\n",
    "\t\t\n",
    "\t## FIX / SYMBOL\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('b/c', ' because ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('b/out', ' break out ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('b/o', ' break out ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('p/e', ' pe ratio ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' [/]+ ', ' ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 1/2 ', ' .5 ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 1/4 ', ' .25 ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 3/4 ', ' .75 ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 1/3 ', ' .3 ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 2/3 ', ' .6 ', regex=False)\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[/]{2,}', ' ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z])/([a-z])', r'\\1 and \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[0-9]+/[0-9]+/[0-9]+', '', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]{3,})/([0-9\\.]{2,})', r'\\1 to \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]{2,})/([0-9\\.]{3,})', r'\\1 to \\2', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[a-z0-9]+/[a-z0-9]+', ' ', regex=True)\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('/', '', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX < > SYMBOLS\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[<]+ ', ' ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('<', ' less than ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' [>]+', ' ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('>', ' greater than ', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX : SYMBOL\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[0-9]+:[0-9]+am', ' ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[0-9]+:[0-9]', ' ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(':', ' ', regex=False)\n",
    "    \n",
    "    ## FIX \" SYMBOL\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('“', ' ', regex=False)\n",
    "\t\n",
    "    ## FIX UNITS\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('user ', ' ', regex=False)\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]+)dma', r'\\1 displaced moving average ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'dma([0-9]+)', r'\\1 displaced moving average ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]+)sma', r'\\1 simple moving average ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'sma([0-9]+)', r'\\1 simple moving average ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]+)ema', r'\\1 expontential moving average ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'ema([0-9]+)', r'\\1 expontential moving average ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]+)ma', r'\\1 moving average ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'ma([0-9]+)', r'\\1 moving average ', regex=True)\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])mos', r'\\1 months ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])minute', r'\\1 minute ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])minutes', r'\\1 minutes ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])min', r'\\1 minute ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])mins', r'\\1 minutes ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])day', r'\\1 day ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])days', r'\\1 days ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])wk', r'\\1 week ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' wk ', ' week ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' wknd ', ' weekend ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])wks', r'\\1 weeks ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])hours', r'\\1 hours ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])hour', r'\\1 hour ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])yr', r'\\1 year ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])yrs', r'\\1 years ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' yr', ' year ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])am', r'\\1 am ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])pm', r'\\1 pm ', regex=True)\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])est', r'\\1 ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])ish', r'\\1 ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9 ])pts', r'\\1 points ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])x', r'\\1 times ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])th', r'\\1 ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])rd', r'\\1 ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])st', r'\\1 ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])nd', r'\\1 ', regex=True)\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('mrkt', 'market', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' vol ', ' volume ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' ptrend', ' positive trend ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' ppl', ' people ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' pts', ' points ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' pt', ' point ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' l(ol){1,}', ' laugh ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('imho', ' in my opinion ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('prev ', 'previous ', regex=True)\n",
    "\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 1q', ' first quarter ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 2q', ' second quarter ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 3q', ' third quarter ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 4q', ' fourth quarter ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' q1', ' first quarter ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' q2', ' second quarter ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' q3', ' third quarter ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' q4', ' fourth quarter ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 10q ', ' form 10 ', regex=False)\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])million', r'\\1 million ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])mil', r'\\1 million ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' mil ', ' million ', regex=False)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])billion', r'\\1 billion ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])cents', r'\\1 cents ', regex=True)\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])3d', r'\\1 3 dimensional ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])gb', r'\\1 3 gigabytes ', regex=True)\n",
    "\n",
    "\n",
    "\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])c', r'\\1 calls ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])y', r'\\1 year ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])p', r'\\1 puts ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])d', r'\\1 days ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])h', r'\\1 hour ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])s', r'\\1 ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])k1', r'\\1 thousand ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])k', r'\\1 thousand ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])m', r'\\1 million ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])b', r'\\1 billion ', regex=True)\n",
    "\n",
    "\t\t\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])([a-z])', r'\\1 \\2', regex=True)\n",
    "\n",
    "\t## FIX EXTRA SPACES AND ENDING PUNCTUATION\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' +', ' ', regex=True)\n",
    "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.strip(' .!?,)(:-')\n",
    "\n",
    "\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f5f585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\object_array.py:158: FutureWarning: Possible nested set at position 1\n",
      "  pat = re.compile(pat, flags=flags)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Like</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-30 23:48:49+00:00</td>\n",
       "      <td>1608973410788319234</td>\n",
       "      <td>NC State gets worked at Clemson #breaking #bre...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "      <td>nc state gets worked at clemson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-30 23:48:48+00:00</td>\n",
       "      <td>1608973408859049987</td>\n",
       "      <td>Ancelotti: “An unforgettable year is coming to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "      <td>ancelotti an unforgettable year is coming to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30 23:48:47+00:00</td>\n",
       "      <td>1608973405616750592</td>\n",
       "      <td>Nunez helps Liverpool push Leicester past thei...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>267</td>\n",
       "      <td>nunez helps liverpool push leicester past thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-30 23:48:45+00:00</td>\n",
       "      <td>1608973397597499395</td>\n",
       "      <td>Btw $GMX P/E is 13, think about it 🤔\\np.s. SP5...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>btw gmx pe ratio is 13 think about it 🤔 p s sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-30 23:46:31+00:00</td>\n",
       "      <td>1608972835409760256</td>\n",
       "      <td>Your Evening Briefing: S&amp;amp;P 500 Ends a Grim...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3596</td>\n",
       "      <td>your evening briefing s and p 500 ends a grim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218723</th>\n",
       "      <td>2022-07-01 00:19:35+00:00</td>\n",
       "      <td>1542664174198718465</td>\n",
       "      <td>The S&amp;amp;P500 total return of Bloomberg Globa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>the s and p500 total return of bloomberg globa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218724</th>\n",
       "      <td>2022-07-01 00:17:24+00:00</td>\n",
       "      <td>1542663628121317376</td>\n",
       "      <td>Pitch: For 2 and 20, we will match the sp500 r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1230</td>\n",
       "      <td>pitch for 2 and 20 we will match the sp500 ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218725</th>\n",
       "      <td>2022-07-01 00:16:38+00:00</td>\n",
       "      <td>1542663432020938752</td>\n",
       "      <td>I prolly shouldn’t say this, but I underperfor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3218</td>\n",
       "      <td>i prolly shouldn’t say this but i underperform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218726</th>\n",
       "      <td>2022-07-01 00:05:01+00:00</td>\n",
       "      <td>1542660508649525248</td>\n",
       "      <td>Top stocks with TA score trending DOWN (SP500)...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5479</td>\n",
       "      <td>top stocks with ta score trending down sp500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218727</th>\n",
       "      <td>2022-07-01 00:04:25+00:00</td>\n",
       "      <td>1542660357637844993</td>\n",
       "      <td>USA index futures tame. We churn and churn\\n\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "      <td>usa index futures tame we churn and churn spx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218728 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Date                   Id  \\\n",
       "0      2022-12-30 23:48:49+00:00  1608973410788319234   \n",
       "1      2022-12-30 23:48:48+00:00  1608973408859049987   \n",
       "2      2022-12-30 23:48:47+00:00  1608973405616750592   \n",
       "3      2022-12-30 23:48:45+00:00  1608973397597499395   \n",
       "4      2022-12-30 23:46:31+00:00  1608972835409760256   \n",
       "...                          ...                  ...   \n",
       "218723 2022-07-01 00:19:35+00:00  1542664174198718465   \n",
       "218724 2022-07-01 00:17:24+00:00  1542663628121317376   \n",
       "218725 2022-07-01 00:16:38+00:00  1542663432020938752   \n",
       "218726 2022-07-01 00:05:01+00:00  1542660508649525248   \n",
       "218727 2022-07-01 00:04:25+00:00  1542660357637844993   \n",
       "\n",
       "                                                    Tweet  Like  Retweet  \\\n",
       "0       NC State gets worked at Clemson #breaking #bre...     0        0   \n",
       "1       Ancelotti: “An unforgettable year is coming to...     0        0   \n",
       "2       Nunez helps Liverpool push Leicester past thei...     0        1   \n",
       "3       Btw $GMX P/E is 13, think about it 🤔\\np.s. SP5...     0        0   \n",
       "4       Your Evening Briefing: S&amp;P 500 Ends a Grim...     1        0   \n",
       "...                                                   ...   ...      ...   \n",
       "218723  The S&amp;P500 total return of Bloomberg Globa...     0        0   \n",
       "218724  Pitch: For 2 and 20, we will match the sp500 r...     0        0   \n",
       "218725  I prolly shouldn’t say this, but I underperfor...     0        0   \n",
       "218726  Top stocks with TA score trending DOWN (SP500)...     0        0   \n",
       "218727  USA index futures tame. We churn and churn\\n\\n...     0        0   \n",
       "\n",
       "        Followers                                       Text_Cleaned  \n",
       "0             267                    nc state gets worked at clemson  \n",
       "1             267  ancelotti an unforgettable year is coming to a...  \n",
       "2             267  nunez helps liverpool push leicester past thei...  \n",
       "3             280  btw gmx pe ratio is 13 think about it 🤔 p s sp...  \n",
       "4            3596  your evening briefing s and p 500 ends a grim ...  \n",
       "...           ...                                                ...  \n",
       "218723         31  the s and p500 total return of bloomberg globa...  \n",
       "218724       1230  pitch for 2 and 20 we will match the sp500 ret...  \n",
       "218725       3218  i prolly shouldn’t say this but i underperform...  \n",
       "218726       5479       top stocks with ta score trending down sp500  \n",
       "218727        217      usa index futures tame we churn and churn spx  \n",
       "\n",
       "[218728 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process the Tweets for NLP\n",
    "data = Preprocess_Tweets(data)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81eb8c9-eeb1-4a29-87d5-61d4dd6ffe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc88a99c-cbb6-491c-ab73-8c3032d8afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Vader sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Predict sentiment with Vader classifier\n",
    "data['Vader_Scores'] = data['Text_Cleaned'].apply(lambda score: sid.polarity_scores(score)['compound'])\n",
    "data['Vader_Prediction'] = data['Vader_Scores'].apply(lambda score: 1 if score >=0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e396bdf-574a-412f-802e-0e5941827a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop('Vader_Scores', axis=1)\n",
    "data = data.rename(columns={'Vader_Prediction':'Sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef2f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create list of custom stop words to remove\n",
    "StopWords = set([s.replace(\"'\", '') for s in stopwords.words('english') if s not in ['not', 'up', 'down', 'above', 'below', 'under', 'over']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6647fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words for tweets\n",
    "data['Text_Processed'] = data['Text_Cleaned'].apply(lambda s: \" \".join([word for word in s.split() if word not in StopWords]))\n",
    "data['Text_Processed'] = data['Text_Processed'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43aab18",
   "metadata": {},
   "outputs": [],
   "source": [
    " data = data.loc[(data['Date']>='2022-10-31') & (data['Date']<='2022-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53325500-1ac0-406f-847d-df492699b072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Like</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Text_Processed</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-30 23:48:49+00:00</td>\n",
       "      <td>1608973410788319234</td>\n",
       "      <td>NC State gets worked at Clemson #breaking #bre...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "      <td>nc state gets worked at clemson</td>\n",
       "      <td>nc state gets worked clemson</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-30 23:48:48+00:00</td>\n",
       "      <td>1608973408859049987</td>\n",
       "      <td>Ancelotti: “An unforgettable year is coming to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "      <td>ancelotti an unforgettable year is coming to a...</td>\n",
       "      <td>ancelotti unforgettable year coming end style”</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30 23:48:47+00:00</td>\n",
       "      <td>1608973405616750592</td>\n",
       "      <td>Nunez helps Liverpool push Leicester past thei...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>267</td>\n",
       "      <td>nunez helps liverpool push leicester past thei...</td>\n",
       "      <td>nunez helps liverpool push leicester past brea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-30 23:48:45+00:00</td>\n",
       "      <td>1608973397597499395</td>\n",
       "      <td>Btw $GMX P/E is 13, think about it 🤔\\np.s. SP5...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>btw gmx pe ratio is 13 think about it 🤔 p s sp...</td>\n",
       "      <td>btw gmx pe ratio 13 think 🤔 p sp500 average pe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-30 23:46:31+00:00</td>\n",
       "      <td>1608972835409760256</td>\n",
       "      <td>Your Evening Briefing: S&amp;amp;P 500 Ends a Grim...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3596</td>\n",
       "      <td>your evening briefing s and p 500 ends a grim ...</td>\n",
       "      <td>evening briefing p 500 ends grim year down 20 ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145976</th>\n",
       "      <td>2022-10-31 00:13:10+00:00</td>\n",
       "      <td>1586873882425761793</td>\n",
       "      <td>Instant Analysis: Giants fall to Seahawks, ent...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>instant analysis giants fall to seahawks enter...</td>\n",
       "      <td>instant analysis giants fall seahawks enter by...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145977</th>\n",
       "      <td>2022-10-31 00:13:10+00:00</td>\n",
       "      <td>1586873879502503937</td>\n",
       "      <td>How to Watch The Simpsons' Death Note Episode ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>how to watch the simpsons death note episode</td>\n",
       "      <td>watch simpsons death note episode</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145978</th>\n",
       "      <td>2022-10-31 00:13:09+00:00</td>\n",
       "      <td>1586873877132562432</td>\n",
       "      <td>Henry runs for 219 yards, 2 TDs as Titans down...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>henry runs for 219 yards 2 tds as titans down ...</td>\n",
       "      <td>henry runs 219 yards 2 tds titans down texans ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145979</th>\n",
       "      <td>2022-10-31 00:06:24+00:00</td>\n",
       "      <td>1586872176820502529</td>\n",
       "      <td>#dailyupdate #sp500 #wallstreet #markettiming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>346</td>\n",
       "      <td>e mini s and p 500 es dec it was d...</td>\n",
       "      <td>e mini p 500 es dec daring make prediction es ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145980</th>\n",
       "      <td>2022-10-31 00:00:00+00:00</td>\n",
       "      <td>1586870565255745536</td>\n",
       "      <td>Elliott Wave Analysis: SP500 Wave v) of C of (...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20010</td>\n",
       "      <td>elliott wave analysis sp500 wave v of c of 2 v...</td>\n",
       "      <td>elliott wave analysis sp500 wave v c 2 video</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78865 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Date                   Id  \\\n",
       "0      2022-12-30 23:48:49+00:00  1608973410788319234   \n",
       "1      2022-12-30 23:48:48+00:00  1608973408859049987   \n",
       "2      2022-12-30 23:48:47+00:00  1608973405616750592   \n",
       "3      2022-12-30 23:48:45+00:00  1608973397597499395   \n",
       "4      2022-12-30 23:46:31+00:00  1608972835409760256   \n",
       "...                          ...                  ...   \n",
       "145976 2022-10-31 00:13:10+00:00  1586873882425761793   \n",
       "145977 2022-10-31 00:13:10+00:00  1586873879502503937   \n",
       "145978 2022-10-31 00:13:09+00:00  1586873877132562432   \n",
       "145979 2022-10-31 00:06:24+00:00  1586872176820502529   \n",
       "145980 2022-10-31 00:00:00+00:00  1586870565255745536   \n",
       "\n",
       "                                                    Tweet  Like  Retweet  \\\n",
       "0       NC State gets worked at Clemson #breaking #bre...     0        0   \n",
       "1       Ancelotti: “An unforgettable year is coming to...     0        0   \n",
       "2       Nunez helps Liverpool push Leicester past thei...     0        1   \n",
       "3       Btw $GMX P/E is 13, think about it 🤔\\np.s. SP5...     0        0   \n",
       "4       Your Evening Briefing: S&amp;P 500 Ends a Grim...     1        0   \n",
       "...                                                   ...   ...      ...   \n",
       "145976  Instant Analysis: Giants fall to Seahawks, ent...     0        0   \n",
       "145977  How to Watch The Simpsons' Death Note Episode ...     0        0   \n",
       "145978  Henry runs for 219 yards, 2 TDs as Titans down...     0        0   \n",
       "145979  #dailyupdate #sp500 #wallstreet #markettiming ...     1        0   \n",
       "145980  Elliott Wave Analysis: SP500 Wave v) of C of (...     1        0   \n",
       "\n",
       "        Followers                                       Text_Cleaned  \\\n",
       "0             267                    nc state gets worked at clemson   \n",
       "1             267  ancelotti an unforgettable year is coming to a...   \n",
       "2             267  nunez helps liverpool push leicester past thei...   \n",
       "3             280  btw gmx pe ratio is 13 think about it 🤔 p s sp...   \n",
       "4            3596  your evening briefing s and p 500 ends a grim ...   \n",
       "...           ...                                                ...   \n",
       "145976        268  instant analysis giants fall to seahawks enter...   \n",
       "145977        268       how to watch the simpsons death note episode   \n",
       "145978        268  henry runs for 219 yards 2 tds as titans down ...   \n",
       "145979        346              e mini s and p 500 es dec it was d...   \n",
       "145980      20010  elliott wave analysis sp500 wave v of c of 2 v...   \n",
       "\n",
       "                                           Text_Processed  Sentiment  \n",
       "0                            nc state gets worked clemson          1  \n",
       "1          ancelotti unforgettable year coming end style”          1  \n",
       "2       nunez helps liverpool push leicester past brea...          1  \n",
       "3       btw gmx pe ratio 13 think 🤔 p sp500 average pe...          1  \n",
       "4       evening briefing p 500 ends grim year down 20 ...         -1  \n",
       "...                                                   ...        ...  \n",
       "145976  instant analysis giants fall seahawks enter by...          1  \n",
       "145977                  watch simpsons death note episode         -1  \n",
       "145978  henry runs 219 yards 2 tds titans down texans ...          1  \n",
       "145979  e mini p 500 es dec daring make prediction es ...          1  \n",
       "145980       elliott wave analysis sp500 wave v c 2 video          1  \n",
       "\n",
       "[78865 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b815acc2-95a1-4f50-9296-71f9e7a83865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and test data into 80/20 split\n",
    "train_pct = .8\n",
    "np.random.seed(1)\n",
    "idx = np.random.permutation(len(data))\n",
    "\n",
    "X_train = data['Text_Processed'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train = data['Sentiment'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = data['Text_Processed'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test = data['Sentiment'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test[y_test==-1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38be988",
   "metadata": {},
   "source": [
    "## Bert Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db51509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Bert NLP model tokenizer to encode tweets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec5ca94-34d2-4bb7-8977-756d4a4e33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine max length for encoding\n",
    "# encoded = [tokenizer.encode(sent, add_special_tokens=True) for sent in data['Text_Processed'].values]\n",
    "# MAX_LEN = max([len(sent) for sent in encoded])\n",
    "# print('Max length: ', MAX_LEN)\n",
    "MAX_LEN=148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177bd785-740d-4328-83ab-638714e5d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the tweets for Bert model\n",
    "def preprocessing_for_bert(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # For each tweet\n",
    "    for line in data:\n",
    "        # encode the data. Return input encoding and attention mask\n",
    "        encoding = tokenizer.encode_plus(\n",
    "                text=line, # data to process\n",
    "                add_special_tokens=True, # adds special chars [CLS] and [SEP] to encoding \n",
    "                padding='max_length', # pad the tweets with 0s to fit max length\n",
    "                max_length = MAX_LEN, # assign max length\n",
    "                truncation=True, # truncate tweets longer than max length\n",
    "                return_tensors=\"pt\", # return tensor as pytorch tensor\n",
    "                return_attention_mask=True # return the attention mask\n",
    "                )\n",
    "\n",
    "        # add the encodings to the list\n",
    "        input_ids.append(encoding.get('input_ids'))\n",
    "        attention_masks.append(encoding.get('attention_mask'))\n",
    "    \n",
    "    # return the lists as tensors\n",
    "    input_ids = torch.concat(input_ids)\n",
    "    attention_masks = torch.concat(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b883c52-8ca4-4a2e-bace-222c0980bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bert NLP Classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        # Define the neurons for the final layer\n",
    "        input_layer = 768\n",
    "        hidden_layer = 50\n",
    "        output_layer = 2\n",
    "\n",
    "        # Use the pretrained Bert model for first section of NN\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Define a final layer to attach to the Bert model for custom classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_layer, hidden_layer), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_layer, output_layer))\n",
    "\n",
    "        # Freeze the model from updating\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    # Return classification from Bert model \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        h_cls = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(h_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec431304-d404-4979-addf-184ee698a733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63092, 148]) torch.Size([63092, 148]) torch.Size([63092])\n",
      "torch.Size([15773, 148]) torch.Size([15773, 148]) torch.Size([15773])\n"
     ]
    }
   ],
   "source": [
    "# Encode the train and test data for Bert\n",
    "X_train_inputs, X_train_masks = preprocessing_for_bert(X_train)\n",
    "X_test_inputs, X_test_masks = preprocessing_for_bert(X_test)\n",
    "\n",
    "# Get the train and test labels\n",
    "y_train_labels = torch.tensor(y_train)\n",
    "y_test_labels = torch.tensor(y_test)\n",
    "\n",
    "print(X_train_inputs.shape, X_train_masks.shape, y_train_labels.shape)\n",
    "print(X_test_inputs.shape, X_test_masks.shape, y_test_labels.shape)\n",
    "\n",
    "# Set batch size to 16. recommended 16 or 32 depending on GPU size \n",
    "batch_size = 16\n",
    "\n",
    "# Randomize the train data and define dataloader for model training \n",
    "train_data = TensorDataset(X_train_inputs, X_train_masks, y_train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Randomize the test data and define dataloader for model testing\n",
    "test_data = TensorDataset(X_test_inputs, X_test_masks, y_test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7d1a64f-19d7-46d5-acd7-b7eafa41b5c1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for repeatability\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Check if GPU is available and assign device \n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "# Initialize Bert Classifier\n",
    "model = BertClassifier(freeze=False)\n",
    "\n",
    "# Send model to device (GPU if available)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140cc264-01bd-4c08-adfb-cde5a5a1c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparameters\n",
    "epochs = 4\n",
    "steps = len(train_dataloader) * epochs\n",
    "learning_rate = 5e-5\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Define Adam optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "# Define scheduler for training the optimizer \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
    "\n",
    "# Define cross entropy loss function \n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "902c0326-57b3-4495-bdf8-777f43614f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  |  Train Loss: 0.20956  |  Test Loss: 0.14960  |  Test Accuracy: 95.74\n",
      "Epoch: 2  |  Train Loss: 0.08687  |  Test Loss: 0.10034  |  Test Accuracy: 97.64\n",
      "Epoch: 3  |  Train Loss: 0.03559  |  Test Loss: 0.09699  |  Test Accuracy: 98.42\n",
      "Epoch: 4  |  Train Loss: 0.01392  |  Test Loss: 0.09399  |  Test Accuracy: 98.59\n"
     ]
    }
   ],
   "source": [
    "# For the number of epochs\n",
    "for e in range(epochs):\n",
    "    # Assign model to train\n",
    "    model.train()\n",
    "\n",
    "    # Intialize loss to zero\n",
    "    train_loss = 0\n",
    "    \n",
    "    # For each batch\n",
    "    for batch in train_dataloader:\n",
    "        # Get batch inputs, masks and labels \n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        \n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Reset the model gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get classification of encoded values\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "        \n",
    "        # Calculate loss based on predictions and known values\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        \n",
    "        # Add loss to the running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update the model weights based on the loss \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over batch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Assign the model to evaluate    \n",
    "    model.eval()\n",
    "\n",
    "    # Initialize losses\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    # For each batch\n",
    "    for batch in test_dataloader:\n",
    "        # Get encoding inputs, masks and labels\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        \n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Predict the input values without updating the model \n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to 0 and 1\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate accuracy of model on test data \n",
    "        accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "        test_acc += accuracy\n",
    "\n",
    "    # Calculate average loss and accuracy per each batch\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    # Print epoch information \n",
    "    print('Epoch: %d  |  Train Loss: %1.5f  |  Test Loss: %1.5f  |  Test Accuracy: %1.2f'%(e+1, train_loss, test_loss, test_acc))\n",
    "    \n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'stock_sentiment_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759d106",
   "metadata": {},
   "source": [
    "## Bert Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65edf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.read_pickle('tweets_01_01_03_31.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e03f3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\object_array.py:158: FutureWarning: Possible nested set at position 1\n",
      "  pat = re.compile(pat, flags=flags)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Like</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-30 23:59:43+00:00</td>\n",
       "      <td>1641591062845063172</td>\n",
       "      <td>Excited for a new month! April 2023 is just ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>excited for a new month april 2023 is just aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-30 23:56:56+00:00</td>\n",
       "      <td>1641590365185605633</td>\n",
       "      <td>LIVE streaming FREE #GOLD #XAUUSD SIGNALS\\nPle...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>211</td>\n",
       "      <td>live streaming free signals please like and su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-30 23:54:04+00:00</td>\n",
       "      <td>1641589643324915713</td>\n",
       "      <td>$DXY:\\n\\nBearish Head &amp;amp; Shoulders pattern ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>337</td>\n",
       "      <td>dxy bearish head and shoulders pattern forming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-30 23:53:03+00:00</td>\n",
       "      <td>1641589387682263040</td>\n",
       "      <td>【🇺🇸S&amp;amp;P500】 +0.57% (+23.02) 4,050.83  https...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1025</td>\n",
       "      <td>🇺🇸s and p500 positive 0.57 percent positive 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-30 23:49:47+00:00</td>\n",
       "      <td>1641588562448752640</td>\n",
       "      <td>@Ben_Egli @GovTinaKotek Cool. What will the SP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>cool what will the sp500 close at tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57067</th>\n",
       "      <td>2023-01-01 00:08:06+00:00</td>\n",
       "      <td>1609340652885524482</td>\n",
       "      <td>'The other team is a professional football tea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>the other team is a professional football team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57068</th>\n",
       "      <td>2023-01-01 00:08:05+00:00</td>\n",
       "      <td>1609340650687709184</td>\n",
       "      <td>Jessie James Decker Shares Stunning Snaps From...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>jessie james decker shares stunning snaps from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57069</th>\n",
       "      <td>2023-01-01 00:07:20+00:00</td>\n",
       "      <td>1609340459129749505</td>\n",
       "      <td>@SamanthaLaDuc All above 3400. Nice.\\n\\nWe hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>all above 3400 nice we have a recession coming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57070</th>\n",
       "      <td>2023-01-01 00:06:19+00:00</td>\n",
       "      <td>1609340203076046850</td>\n",
       "      <td>@CurtisEyiogbe @mbontrager5 -8% for SCV vs -15...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2891</td>\n",
       "      <td>negative 8 percent for scv vs negative 15 perc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57071</th>\n",
       "      <td>2023-01-01 00:03:34+00:00</td>\n",
       "      <td>1609339512890482688</td>\n",
       "      <td>40% cash waiting patiently for sp500 3300.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2195</td>\n",
       "      <td>40 percent cash waiting patiently for sp500 3300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57072 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Date                   Id  \\\n",
       "0     2023-03-30 23:59:43+00:00  1641591062845063172   \n",
       "1     2023-03-30 23:56:56+00:00  1641590365185605633   \n",
       "2     2023-03-30 23:54:04+00:00  1641589643324915713   \n",
       "3     2023-03-30 23:53:03+00:00  1641589387682263040   \n",
       "4     2023-03-30 23:49:47+00:00  1641588562448752640   \n",
       "...                         ...                  ...   \n",
       "57067 2023-01-01 00:08:06+00:00  1609340652885524482   \n",
       "57068 2023-01-01 00:08:05+00:00  1609340650687709184   \n",
       "57069 2023-01-01 00:07:20+00:00  1609340459129749505   \n",
       "57070 2023-01-01 00:06:19+00:00  1609340203076046850   \n",
       "57071 2023-01-01 00:03:34+00:00  1609339512890482688   \n",
       "\n",
       "                                                   Tweet  Like  Retweet  \\\n",
       "0      Excited for a new month! April 2023 is just ar...     0        0   \n",
       "1      LIVE streaming FREE #GOLD #XAUUSD SIGNALS\\nPle...     0        1   \n",
       "2      $DXY:\\n\\nBearish Head &amp; Shoulders pattern ...     3        0   \n",
       "3      【🇺🇸S&amp;P500】 +0.57% (+23.02) 4,050.83  https...     0        0   \n",
       "4      @Ben_Egli @GovTinaKotek Cool. What will the SP...     0        0   \n",
       "...                                                  ...   ...      ...   \n",
       "57067  'The other team is a professional football tea...     0        0   \n",
       "57068  Jessie James Decker Shares Stunning Snaps From...     0        0   \n",
       "57069  @SamanthaLaDuc All above 3400. Nice.\\n\\nWe hav...     0        0   \n",
       "57070  @CurtisEyiogbe @mbontrager5 -8% for SCV vs -15...     0        0   \n",
       "57071         40% cash waiting patiently for sp500 3300.     0        0   \n",
       "\n",
       "       Followers                                       Text_Cleaned  \n",
       "0            490  excited for a new month april 2023 is just aro...  \n",
       "1            211  live streaming free signals please like and su...  \n",
       "2            337  dxy bearish head and shoulders pattern forming...  \n",
       "3           1025  🇺🇸s and p500 positive 0.57 percent positive 23...  \n",
       "4             46         cool what will the sp500 close at tomorrow  \n",
       "...          ...                                                ...  \n",
       "57067        269  the other team is a professional football team...  \n",
       "57068        269  jessie james decker shares stunning snaps from...  \n",
       "57069         35  all above 3400 nice we have a recession coming...  \n",
       "57070       2891  negative 8 percent for scv vs negative 15 perc...  \n",
       "57071       2195   40 percent cash waiting patiently for sp500 3300  \n",
       "\n",
       "[57072 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock = Preprocess_Tweets(stock)\n",
    "display(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc009d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22724\\3588551695.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prepare the Bert NLP model tokenizer to encode tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m148\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare the Bert NLP model tokenizer to encode tweets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "MAX_LEN=148\n",
    "\n",
    "model = BertClassifier()\n",
    "model.load_state_dict(torch.load('stock_sentiment_model.pt'))\n",
    "\n",
    "# Check if GPU is available and assign device \n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "# Send model to device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "stock_inputs, stock_masks = preprocessing_for_bert(stock['Text_Cleaned'].values)\n",
    "\n",
    "batch_size = 16\n",
    " \n",
    "stock_data = TensorDataset(stock_inputs, stock_masks)\n",
    "stock_sampler = RandomSampler(stock_data)\n",
    "stock_dataloader = DataLoader(stock_data, sampler=stock_sampler, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "# For each batch\n",
    "for batch in stock_dataloader:\n",
    "    # Get encoding inputs, masks and labels\n",
    "    batch_inputs, batch_masks = batch\n",
    "\n",
    "    # Send variables to device (GPU if available)\n",
    "    batch_inputs = batch_inputs.to(device)\n",
    "    batch_masks = batch_masks.to(device)\n",
    "\n",
    "    # Predict the input values without updating the model \n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "    # Convert predictions to 0 and 1\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    predictions.append(preds)\n",
    "\n",
    "# Combine all batch predictions\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "\n",
    "# Add predictions to stock dataframe\n",
    "stock['Sentiment'] = predictions\n",
    "\n",
    "stock.to_pickle('stock_data_sentiment.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf2317-f6db-4f82-ab4d-39fa8cb6a081",
   "metadata": {},
   "source": [
    "## Predict the stock market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfa5d5c3-17c6-441b-9d15-ef20d3959f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Like</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-30 23:59:43+00:00</td>\n",
       "      <td>1641591062845063172</td>\n",
       "      <td>Excited for a new month! April 2023 is just ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>excited for a new month april 2023 is just aro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-30 23:56:56+00:00</td>\n",
       "      <td>1641590365185605633</td>\n",
       "      <td>LIVE streaming FREE #GOLD #XAUUSD SIGNALS\\nPle...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>211</td>\n",
       "      <td>live streaming free signals please like and su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-30 23:54:04+00:00</td>\n",
       "      <td>1641589643324915713</td>\n",
       "      <td>$DXY:\\n\\nBearish Head &amp;amp; Shoulders pattern ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>337</td>\n",
       "      <td>dxy bearish head and shoulders pattern forming...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-30 23:53:03+00:00</td>\n",
       "      <td>1641589387682263040</td>\n",
       "      <td>【🇺🇸S&amp;amp;P500】 +0.57% (+23.02) 4,050.83  https...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1025</td>\n",
       "      <td>🇺🇸s and p500 positive 0.57 percent positive 23...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-30 23:49:47+00:00</td>\n",
       "      <td>1641588562448752640</td>\n",
       "      <td>@Ben_Egli @GovTinaKotek Cool. What will the SP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>cool what will the sp500 close at tomorrow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57067</th>\n",
       "      <td>2023-01-01 00:08:06+00:00</td>\n",
       "      <td>1609340652885524482</td>\n",
       "      <td>'The other team is a professional football tea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>the other team is a professional football team...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57068</th>\n",
       "      <td>2023-01-01 00:08:05+00:00</td>\n",
       "      <td>1609340650687709184</td>\n",
       "      <td>Jessie James Decker Shares Stunning Snaps From...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>jessie james decker shares stunning snaps from...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57069</th>\n",
       "      <td>2023-01-01 00:07:20+00:00</td>\n",
       "      <td>1609340459129749505</td>\n",
       "      <td>@SamanthaLaDuc All above 3400. Nice.\\n\\nWe hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>all above 3400 nice we have a recession coming...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57070</th>\n",
       "      <td>2023-01-01 00:06:19+00:00</td>\n",
       "      <td>1609340203076046850</td>\n",
       "      <td>@CurtisEyiogbe @mbontrager5 -8% for SCV vs -15...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2891</td>\n",
       "      <td>negative 8 percent for scv vs negative 15 perc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57071</th>\n",
       "      <td>2023-01-01 00:03:34+00:00</td>\n",
       "      <td>1609339512890482688</td>\n",
       "      <td>40% cash waiting patiently for sp500 3300.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2195</td>\n",
       "      <td>40 percent cash waiting patiently for sp500 3300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57072 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Date                   Id  \\\n",
       "0     2023-03-30 23:59:43+00:00  1641591062845063172   \n",
       "1     2023-03-30 23:56:56+00:00  1641590365185605633   \n",
       "2     2023-03-30 23:54:04+00:00  1641589643324915713   \n",
       "3     2023-03-30 23:53:03+00:00  1641589387682263040   \n",
       "4     2023-03-30 23:49:47+00:00  1641588562448752640   \n",
       "...                         ...                  ...   \n",
       "57067 2023-01-01 00:08:06+00:00  1609340652885524482   \n",
       "57068 2023-01-01 00:08:05+00:00  1609340650687709184   \n",
       "57069 2023-01-01 00:07:20+00:00  1609340459129749505   \n",
       "57070 2023-01-01 00:06:19+00:00  1609340203076046850   \n",
       "57071 2023-01-01 00:03:34+00:00  1609339512890482688   \n",
       "\n",
       "                                                   Tweet  Like  Retweet  \\\n",
       "0      Excited for a new month! April 2023 is just ar...     0        0   \n",
       "1      LIVE streaming FREE #GOLD #XAUUSD SIGNALS\\nPle...     0        1   \n",
       "2      $DXY:\\n\\nBearish Head &amp; Shoulders pattern ...     3        0   \n",
       "3      【🇺🇸S&amp;P500】 +0.57% (+23.02) 4,050.83  https...     0        0   \n",
       "4      @Ben_Egli @GovTinaKotek Cool. What will the SP...     0        0   \n",
       "...                                                  ...   ...      ...   \n",
       "57067  'The other team is a professional football tea...     0        0   \n",
       "57068  Jessie James Decker Shares Stunning Snaps From...     0        0   \n",
       "57069  @SamanthaLaDuc All above 3400. Nice.\\n\\nWe hav...     0        0   \n",
       "57070  @CurtisEyiogbe @mbontrager5 -8% for SCV vs -15...     0        0   \n",
       "57071         40% cash waiting patiently for sp500 3300.     0        0   \n",
       "\n",
       "       Followers                                       Text_Cleaned  Sentiment  \n",
       "0            490  excited for a new month april 2023 is just aro...          1  \n",
       "1            211  live streaming free signals please like and su...          0  \n",
       "2            337  dxy bearish head and shoulders pattern forming...          1  \n",
       "3           1025  🇺🇸s and p500 positive 0.57 percent positive 23...          1  \n",
       "4             46         cool what will the sp500 close at tomorrow          1  \n",
       "...          ...                                                ...        ...  \n",
       "57067        269  the other team is a professional football team...          1  \n",
       "57068        269  jessie james decker shares stunning snaps from...          1  \n",
       "57069         35  all above 3400 nice we have a recession coming...          1  \n",
       "57070       2891  negative 8 percent for scv vs negative 15 perc...          1  \n",
       "57071       2195   40 percent cash waiting patiently for sp500 3300          1  \n",
       "\n",
       "[57072 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "\n",
    "stock.loc[stock['Sentiment'] == 0, 'Sentiment'] = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d854cb9e-ade5-459e-a57e-3203b47c4171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
